{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d337534-4b82-414a-aaad-98fc495ce741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import shap\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import regularizers\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "    \n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eebb65e-954d-4f38-a8b8-60d8627f46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir=('/Users/maddie/Projects/CPSC_597/')\n",
    "#create text file to store results in and close again:\n",
    "with open(file_dir+'/results/DNN_performance/DNN_eval_future.txt','w+') as file:\n",
    "    file.write(\"Species\"+\"\\t\"+\"Test_loss\"+\"\\t\"+\"Test_acc\"+\"\\t\"+\"Test_tpr\"+\"\\t\"+\"Test_AUC\"+\"\\t\"+\"Test_LCI95%\"+\"\\t\"+\"Test_UCI95%\"+\"\\t\"+\"occ_samples\"+\"\\t\"+\"abs_samples\"+\"\\n\")\n",
    "    file.close()\n",
    "#access file with list of taxa names\n",
    "taxa=pd.read_csv(file_dir+'/data/modified_data/gbif_filtered/taxa_list.txt',header=None)\n",
    "taxa.columns=[\"taxon\"] \n",
    "\n",
    "###column variable names\n",
    "with open(file_dir+'data/data_raw/bio_oracle_future/variable_list.txt') as f:\n",
    "      new_cols = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d53731-4b59-46b2-ad16-72bcee4abd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1\n",
      "191/191 [==============================] - 0s 945us/step - loss: 0.4370 - accuracy: 0.8128\n",
      "191/191 [==============================] - 0s 948us/step\n",
      "run 2\n",
      "191/191 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.8128\n",
      "191/191 [==============================] - 0s 1ms/step\n",
      "run 3\n",
      "191/191 [==============================] - 0s 995us/step - loss: 0.4467 - accuracy: 0.8128\n",
      "191/191 [==============================] - 0s 748us/step\n",
      "run 4\n",
      "191/191 [==============================] - 0s 908us/step - loss: 0.4258 - accuracy: 0.8128\n",
      "191/191 [==============================] - 0s 744us/step\n",
      "run 5\n",
      "191/191 [==============================] - 0s 988us/step - loss: 0.4306 - accuracy: 0.8128\n",
      "191/191 [==============================] - 0s 886us/step\n",
      "run 1\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8897\n",
      "197/197 [==============================] - 0s 684us/step\n",
      "run 2\n",
      "197/197 [==============================] - 0s 770us/step - loss: 0.3151 - accuracy: 0.8897\n",
      "197/197 [==============================] - 0s 752us/step\n",
      "run 3\n",
      "197/197 [==============================] - 0s 810us/step - loss: 0.2923 - accuracy: 0.8897\n",
      "197/197 [==============================] - 0s 1ms/step\n",
      "run 4\n",
      "197/197 [==============================] - 0s 851us/step - loss: 0.3176 - accuracy: 0.8897\n",
      "197/197 [==============================] - 0s 805us/step\n",
      "run 5\n",
      "197/197 [==============================] - 0s 1ms/step - loss: 0.3180 - accuracy: 0.8897\n",
      "197/197 [==============================] - 0s 1ms/step\n",
      "run 1\n",
      "123/123 [==============================] - 0s 959us/step - loss: 0.3369 - accuracy: 0.8831\n",
      "123/123 [==============================] - 0s 696us/step\n",
      "run 2\n",
      "123/123 [==============================] - 0s 985us/step - loss: 0.3386 - accuracy: 0.8831\n",
      "123/123 [==============================] - 0s 763us/step\n",
      "run 3\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8831\n",
      "123/123 [==============================] - 0s 905us/step\n",
      "run 4\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.3401 - accuracy: 0.8831\n",
      "123/123 [==============================] - 0s 912us/step\n",
      "run 5\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.3367 - accuracy: 0.8831\n",
      "123/123 [==============================] - 0s 1ms/step\n",
      "run 1\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.4280 - accuracy: 0.8385\n",
      "144/144 [==============================] - 0s 973us/step\n",
      "run 2\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.4258 - accuracy: 0.8407\n",
      "144/144 [==============================] - 0s 874us/step\n",
      "run 3\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.4258 - accuracy: 0.8381\n",
      "144/144 [==============================] - 0s 845us/step\n",
      "run 4\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.4650 - accuracy: 0.7881\n",
      "144/144 [==============================] - 0s 1ms/step\n",
      "run 5\n",
      "144/144 [==============================] - 0s 1ms/step - loss: 0.4845 - accuracy: 0.7881\n",
      "144/144 [==============================] - 0s 1ms/step\n",
      "run 1\n",
      "267/267 [==============================] - 0s 787us/step - loss: 0.5347 - accuracy: 0.7608\n",
      "267/267 [==============================] - 0s 881us/step\n",
      "run 2\n",
      "267/267 [==============================] - 0s 939us/step - loss: 0.5345 - accuracy: 0.7607\n",
      "267/267 [==============================] - 0s 825us/step\n",
      "run 3\n",
      "267/267 [==============================] - 0s 935us/step - loss: 0.5420 - accuracy: 0.7385\n",
      "267/267 [==============================] - 0s 743us/step\n",
      "run 4\n",
      "267/267 [==============================] - 0s 892us/step - loss: 0.5353 - accuracy: 0.7639\n",
      "267/267 [==============================] - 0s 735us/step\n",
      "run 5\n",
      "267/267 [==============================] - 0s 748us/step - loss: 0.5346 - accuracy: 0.7580\n",
      "267/267 [==============================] - 0s 661us/step\n",
      "run 1\n",
      "401/401 [==============================] - 0s 830us/step - loss: 0.6329 - accuracy: 0.6398\n",
      "401/401 [==============================] - 0s 646us/step\n",
      "run 2\n",
      "401/401 [==============================] - 0s 728us/step - loss: 0.6334 - accuracy: 0.6424\n",
      "401/401 [==============================] - 0s 622us/step\n",
      "run 3\n",
      "401/401 [==============================] - 0s 729us/step - loss: 0.6316 - accuracy: 0.6390\n",
      "401/401 [==============================] - 0s 726us/step\n",
      "run 4\n",
      "401/401 [==============================] - 0s 730us/step - loss: 0.6288 - accuracy: 0.6404\n",
      "401/401 [==============================] - 0s 635us/step\n",
      "run 5\n",
      "401/401 [==============================] - 0s 806us/step - loss: 0.6307 - accuracy: 0.6371\n",
      "401/401 [==============================] - 0s 621us/step\n"
     ]
    }
   ],
   "source": [
    "var_names=[]\n",
    "\n",
    "for item in new_cols:\n",
    "    item=item.replace(\"\\n\",\"\")\n",
    "    var_names.append(item) \n",
    "\n",
    "    \n",
    "for species in taxa[\"taxon\"][:]:\n",
    "   \n",
    "    #open dataframe and rename columns\n",
    "    spec = species\n",
    "    table = pd.read_csv(file_dir +\"/data/modified_data/spec_ppa_future_env/%s_future_env_dataframe.csv\"%spec)         \n",
    "    table.rename(columns=dict(zip(table.columns[1:10], var_names)),inplace=True)\n",
    "    \n",
    "    ####################################\n",
    "    #  filter dataframe for training   #\n",
    "    ####################################\n",
    "   \n",
    "    # drop any row with no-data values\n",
    "    table = table.dropna(axis=0, how=\"any\")\n",
    "\n",
    "\n",
    "    # make feature vector\n",
    "    band_columns = [column for column in table.columns[1:10]]\n",
    "    \n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for _, row in table.iterrows():\n",
    "        x = row[band_columns].values\n",
    "        x = x.tolist()\n",
    "        x.append(row[\"present/pseudo_absent\"])\n",
    "        X.append(x)\n",
    "\n",
    "    df = pd.DataFrame(data=X, columns=band_columns + [\"presence\"])\n",
    "    df.to_csv(\"filtered.csv\", index=None)\n",
    "\n",
    "    # extract n. of occ. and abs. samples\n",
    "    occ_len=int(len(df[df[\"presence\" ]==1]))\n",
    "    abs_len=int(len(df[df[\"presence\" ]==0]))\n",
    "    \n",
    "    ####################################\n",
    "    #  Numpy feature and target array  #\n",
    "    ####################################\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    band_columns = [column for column in df.columns[:-1]]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        X.append(row[band_columns].values.tolist())\n",
    "        y.append([1 - row[\"presence\"], row[\"presence\"]])\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y = np.vstack(y)\n",
    "\n",
    "\n",
    "    def makeOverSamplesADASYN(X,y):\n",
    "        #input DataFrame\n",
    "        # #X →Independent Variable in DataFrame\\\n",
    "        # #y →dependent Variable in Pandas DataFrame format\n",
    "         sm = ADASYN()\n",
    "         X, y = sm.fit_sample(X, y)\n",
    "         return(X,y)\n",
    "\n",
    "\n",
    "    ####################################\n",
    "    #    Split training and test set   #\n",
    "    ####################################\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y,random_state=42)\n",
    "    \n",
    "    test_set=pd.DataFrame(X_test)\n",
    "    test_set.rename(columns=dict(zip(test_set.columns[0:9], var_names)),inplace=True)\n",
    "    \n",
    "    #shuffled_X_train=X_train.copy()\n",
    "    #np.random.shuffle(shuffled_X_train)\n",
    "    #shuffled_X_train=shuffled_X_train[:1000] # random subsample from test set for feature importance\n",
    "    \n",
    "#    shuffled_X_test=X_test.copy()\n",
    "#    np.random.shuffle(shuffled_X_test)\n",
    "    #shuffled_X_test=shuffled_X_test[:1000] # random subsample from test set for feature importance\n",
    "    \n",
    "    ####################################\n",
    "    #      Training and testing        #\n",
    "    ####################################\n",
    "    \n",
    "    # prepare metrics\n",
    "    test_loss=[]\n",
    "    test_acc=[]\n",
    "    test_AUC=[]\n",
    "    test_tpr=[]\n",
    "    test_uci=[]\n",
    "    test_lci=[]\n",
    "\n",
    "   \n",
    "    Best_model_AUC=[0]\n",
    "    \n",
    "    # Five repetitions\n",
    "    for i in range(1,6):\n",
    "        print(\"run %s\"%i)\n",
    "        ###################\n",
    "        # Construct model #\n",
    "        ###################\n",
    "        batch_size = 75\n",
    "        num_classes = 2\n",
    "        epochs = 40\n",
    "\n",
    "        num_inputs = X.shape[1]  # number of features\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "        layer_1 = Dense(50, activation='relu',input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.000001))\n",
    "        layer_2 = Dense(25, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.000001))\n",
    "        layer_3 = Dense(25, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.0000001))\n",
    "        layer_4 = Dense(25, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.00000001))\n",
    "\n",
    "\n",
    "        model.add(layer_1)\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(layer_2)\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(layer_3)\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(layer_4)\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "        out_layer = Dense(num_classes, activation=None)\n",
    "        model.add(out_layer)\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics =['accuracy'])\n",
    "        \n",
    "        ###############\n",
    "        # Train model #\n",
    "        ###############\n",
    "        \n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size = batch_size, verbose=0)\n",
    "\n",
    "        ##############\n",
    "        # Test model #\n",
    "        ##############\n",
    "        score = model.evaluate(X_test, y_test, verbose=1)\n",
    "        predictions = model.predict(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:, 1], predictions[:, 1])\n",
    "        len_tpr=int(len(tpr)/2)\n",
    "      \n",
    "\n",
    "        #################\n",
    "        # Append scores #\n",
    "        #################\n",
    "        test_loss.append(score[0])\n",
    "        test_acc.append(score[1])\n",
    "        test_AUC.append(roc_auc_score(y_test[:, 1], predictions[:, 1]))\n",
    "        test_tpr.append(tpr[len_tpr])\n",
    "        AUC = roc_auc_score(y_test[:, 1], predictions[:, 1])\n",
    "\n",
    "        ###############################\n",
    "        # Create confidence intervals #\n",
    "        ###############################\n",
    "        n_bootstraps=1000\n",
    "        y_pred=predictions[:,1]\n",
    "        y_true=y_test[:,1]\n",
    "        rng_seed=42\n",
    "        bootstrapped_scores =[]\n",
    "\n",
    "\n",
    "        rng=np.random.RandomState(rng_seed)\n",
    "        for i in range (n_bootstraps):\n",
    "            #bootstrap by sampling with replacement on prediction indices\n",
    "            indices = rng.randint(0,len(y_pred)-1,len(y_pred))\n",
    "            if len (np.unique(y_true[indices])) <2:\n",
    "                continue\n",
    "\n",
    "            score = roc_auc_score(y_true[indices],y_pred[indices])\n",
    "            bootstrapped_scores.append(score)\n",
    "\n",
    "        sorted_scores=np.array(bootstrapped_scores)\n",
    "        sorted_scores.sort()\n",
    "\n",
    "        ci_lower=sorted_scores[int(0.05*len(sorted_scores))]\n",
    "        ci_upper=sorted_scores[int(0.95*len(sorted_scores))]\n",
    "     \n",
    "        test_lci.append(ci_lower)\n",
    "        test_uci.append(ci_upper)\n",
    "       \n",
    "    \n",
    "        ##############################################################\n",
    "        # Selection of best model across runs and feature importance #\n",
    "        ##############################################################\n",
    "    \n",
    "        #determine whether new model AUC is higher\n",
    "        if AUC > Best_model_AUC[0]:\n",
    "            # if yes save model to disk / overwrite previous model\n",
    "            Best_model_AUC[0]=AUC\n",
    "            model_json=model.to_json()\n",
    "            with open (file_dir+'results/fish_future/{}/{}_model.json'.format(spec,spec),'w') as json_file:\n",
    "                json_file.write(model_json)\n",
    "            model.save_weights(file_dir+'results/fish_future/{}/{}_model.h5'.format(spec,spec))\n",
    "            #if yes, save a figure of shap feature value impact    \n",
    "            \n",
    "           # if int(len(X_train)) > 5000:           \n",
    "           #     explainer=shap.DeepExplainer(model,shuffled_X_train)\n",
    "           #     test_set=pd.DataFrame(shuffled_X_test)\n",
    "           #     test_set.rename(columns=dict(zip(test_set.columns[0:40], var_names)),inplace=True)\n",
    "                \n",
    "           #     shap_values=explainer.shap_values(shuffled_X_test)\n",
    "           #     fig=shap.summary_plot(shap_values[1],test_set,show=False)\n",
    "           #     plt.savefig(file_dir+'/results/fish/{}/{}_feature_impact'.format(spec,spec),bbox_inches=\"tight\")\n",
    "           #     plt.close()\n",
    "            \n",
    "           # else:\n",
    "           #     explainer=shap.DeepExplainer(model,X_train)\n",
    "           #     shap_values=explainer.shap_values(X_test)\n",
    "           #     fig=shap.summary_plot(shap_values[1],test_set,show=False)\n",
    "           #     plt.savefig(file_dir+'/results/fish/{}/{}_feature_impact'.format(spec,spec),bbox_inches=\"tight\")\n",
    "           #     plt.close()\n",
    "            \n",
    "\n",
    "\n",
    "    # Model output metrics averaged across five runs to be written to file\n",
    "    avg_loss= sum(test_loss)/len(test_loss)\n",
    "    avg_acc = sum(test_acc)/len(test_acc)\n",
    "    avg_AUC = sum(test_AUC)/len(test_AUC)\n",
    "    avg_tpr = sum(test_tpr)/len(test_tpr)\n",
    "    avg_lci = sum(test_lci)/len(test_lci)\n",
    "    avg_uci = sum(test_uci)/len(test_uci)\n",
    "\n",
    "    # Write to file\n",
    "    with open(file_dir+'/results/DNN_performance/DNN_eval_future.txt','a') as file:\n",
    "        file.write(spec+\"\\t\"+str(avg_loss)+\"\\t\"+str(avg_acc)+\"\\t\"+str(avg_tpr)+\"\\t\"+str(avg_AUC)+\"\\t\"+str(avg_lci)+\"\\t\"+str(avg_uci)+\"\\t\"+str(occ_len)+\"\\t\"+str(abs_len)+\"\\n\")       \n",
    "\n",
    "\n",
    "    #Next species!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
