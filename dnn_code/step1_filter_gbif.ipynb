{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.affinity import scale\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "from pygbif.species import name_backbone\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "file_dir=('/Users/maddie/Projects/CPSC_597/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8_/ndsj9vc92xq7fmvlqlkg8zt40000gn/T/ipykernel_29456/3907620368.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create the dataframes to be concatenated and filtered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mocc_all_species\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/data_raw/gbif_data_raw/occurrences_all_species.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mocc_all_species\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mocc_all_species\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Get unique label names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5476\u001b[0m         ):\n\u001b[1;32m   5477\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5478\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/strings/accessor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/strings/accessor.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minferred_dtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .str accessor with string values!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "#Create the dataframes to be concatenated and filtered\n",
    "occ_all_species = pd.read_csv(file_dir+\"/data_raw/gbif_data_raw/occurrences_all_species.csv\", low_memory= False)\n",
    "df = occ_all_species[occ_all_species['label'].str.contains(\" \")]\n",
    "\n",
    "#Get unique label names\n",
    "unique_labels=df[\"label\"].unique()\n",
    "\n",
    "names = []\n",
    "back_key =[]\n",
    "remaining_labels=[]\n",
    "\n",
    "#Get backbone associated species names and taxon keys\n",
    "for item in unique_labels:\n",
    "    if \"species\" in name_backbone(item):\n",
    "        i = name_backbone(item)['species']\n",
    "        j = name_backbone(item)['speciesKey']\n",
    "        names.append(i)\n",
    "        back_key.append(j)\n",
    "    else:\n",
    "        remaining_labels.append(item)\n",
    "        \n",
    "for item in remaining_labels:\n",
    "    value=name_backbone(item)['taxonKey']\n",
    "    back_key.append(value)\n",
    "    names.append(item)\n",
    "    \n",
    "#Put into DataFrame\n",
    "df=pd.DataFrame({\"label\": unique_labels,\"back_key\": back_key,\"species\": names},columns=[\"label\",\"back_key\",\"species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 without na's n.rows: 207274\n"
     ]
    }
   ],
   "source": [
    "#Concatenate with occurrence data, dataframe, drop na's \n",
    "df2=pd.merge(occ_all_species,df,how=\"left\",on=\"label\")\n",
    "\n",
    "df2 = df2[pd.notnull(df2['species_x'])]\n",
    "df2 = df2[pd.notnull(df2['decimalLatitude'])]\n",
    "df2 = df2[pd.notnull(df2['decimalLongitude'])]\n",
    "\n",
    "print(\"df2 without na's n.rows:\", len(df2.index))\n",
    "\n",
    "df2[\"back_key\"]=df2[\"back_key\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citharichthys_sordidus 27238\n",
      "Engraulis_mordax 1980\n",
      "Paralichthys_californicus 416\n",
      "Scomber_japonicus 10875\n",
      "Thunnus_alalunga 35146\n",
      "Xiphias_gladius 131619\n"
     ]
    }
   ],
   "source": [
    "#list of species\n",
    "species = df2[\"species_x\"].unique()\n",
    "species.sort()\n",
    "\n",
    "#save separate dataframe for each species as csv file \n",
    "for spec in species:\n",
    "    data=df2.loc[df2['species_x'] == spec]\n",
    "    if len(data.index)>= 10:\n",
    "        spec=spec.replace(\" \",\"_\")\n",
    "        print(\"%s\"%spec, len(data.index))\n",
    "        data.to_csv(file_dir+'/data_raw/gbif_data_raw/%s_gbif_raw.csv'%spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing species Citharichthys_sordidus\n",
      "length only including lon-lat 2 decimals 27192\n",
      "length unique lon-lat 6042\n",
      "processing species Engraulis_mordax\n",
      "length only including lon-lat 2 decimals 1958\n",
      "length unique lon-lat 1588\n",
      "processing species Paralichthys_californicus\n",
      "length only including lon-lat 2 decimals 415\n",
      "length unique lon-lat 403\n",
      "processing species Scomber_japonicus\n",
      "length only including lon-lat 2 decimals 9777\n",
      "length unique lon-lat 2783\n",
      "processing species Thunnus_alalunga\n",
      "length only including lon-lat 2 decimals 24629\n",
      "length unique lon-lat 14749\n",
      "processing species Xiphias_gladius\n",
      "length only including lon-lat 2 decimals 79872\n",
      "length unique lon-lat 50678\n"
     ]
    }
   ],
   "source": [
    "#Open shapefile containing Pacific Ocean range\n",
    "dist = (file_dir+'/data_raw/world-ocean/ne_50m_ocean.shp')\n",
    "dist_shp = gpd.read_file(dist)\n",
    "\n",
    "#create txt file with name of species included after filtering\n",
    "taxa_list=open(file_dir+'/data_raw/gbif_data_raw/taxa_list.txt',\"w\")\n",
    "\n",
    "\n",
    "#Filter occurrences per species\n",
    "for spec in species:\n",
    "    \n",
    "    data=df2.loc[df2['species_x'] == spec] #select subset of species\n",
    "    \n",
    "    # check >10 observations\n",
    "    if len(data.index)>= 10: \n",
    "\n",
    "        spec = spec.replace(\" \",\"_\")\n",
    "        print(\"processing species %s\"%spec)\n",
    "\n",
    "        data=pd.read_csv(file_dir+'/data_raw/gbif_data_raw/%s_gbif_raw.csv'%spec, low_memory= False) #load in data\n",
    "        \n",
    "        ###################################################\n",
    "        # check number of decimals longitude and latitude #\n",
    "        ###################################################\n",
    "        str_lat=(pd.Series.tolist(data[\"decimalLatitude\"].astype(str)))\n",
    "        str_lon=(pd.Series.tolist(data[\"decimalLongitude\"].astype(str)))\n",
    "        dec_lat=[]\n",
    "        dec_lon=[]\n",
    "\n",
    "        for i in range(len(str_lat)):\n",
    "    \n",
    "            if \"e\" in str_lat[i]:\n",
    "                str_lat[i]=\"0.00\"\n",
    "                decla = str_lat[i].split(\".\")[1]\n",
    "                dec_lat.append(int(len(decla)))\n",
    "            else:\n",
    "                decla = str_lat[i].split(\".\")[1]\n",
    "                dec_lat.append(int(len(decla)))\n",
    "                \n",
    "        for i in range(len(str_lon)):\n",
    "            declo=str_lon[i].split(\".\")[1]\n",
    "            dec_lon.append(int(len(declo)))\n",
    "    \n",
    "        data[\"dec_lat\"]=dec_lat\n",
    "        data[\"dec_lon\"]=dec_lon\n",
    "\n",
    "        # filter only include those with min. 2 points\n",
    "        data=data[data[\"dec_lat\"] >= 2]\n",
    "        data=data[data[\"dec_lon\"] >= 2]\n",
    "        print(\"length only including lon-lat 2 decimals\",len(data.index))\n",
    "\n",
    "        data['coordinates'] = list(zip(data[\"decimalLongitude\"], data[\"decimalLatitude\"]))\n",
    "        data['lonlat'] = list(zip(data[\"decimalLongitude\"], data[\"decimalLatitude\"]))\n",
    "        data['coordinates'] = data[\"coordinates\"].apply(Point)\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "        # only keep records with unique lon-lat #\n",
    "        #########################################\n",
    "        \n",
    "        data = data.drop_duplicates('lonlat')\n",
    "        print(\"length unique lon-lat\",len(data.index))\n",
    "\n",
    "       \n",
    "        ###############################################\n",
    "        # only keep records falling in IUNC range #\n",
    "        ###############################################\n",
    "        \n",
    "        #speci=spec.replace(\"_\",\" \")\n",
    "        #dist_shp_spec = dist_shp[dist_shp[\"LAT\"]== \"%s\"%speci]\n",
    "        #poly_spec = dist_shp_spec[[\"LAT\"]]\n",
    "        \n",
    "         #merge the polygons\n",
    "        #iucn_poly_spec= poly_spec.\n",
    "        #Q3 = iucn_poly_spec\n",
    "        #Q3 #inspect polygon\n",
    "\n",
    "        #if Q3.is_valid== False:\n",
    "        #    Q3 = Q3.buffer(0)\n",
    "\n",
    "        #condition_list=[]\n",
    "\n",
    "#        for point in data[\"coordinates\"]:\n",
    "#            output= point.within(poly_spec)\n",
    "#            condition_list.append(output)\n",
    "\n",
    "        #keep records that are in species range\n",
    "#        data[\"in_dist_polygon\"]=condition_list\n",
    "#        data2=data[data.in_dist_polygon == True]\n",
    " #       print(\"length in species dist polygon\",len(data2.index))\n",
    "\n",
    "        #############################\n",
    "        # Only keep records > 1900  #\n",
    "        #############################\n",
    "        \n",
    "    #    data['event_date'] = pd.to_datetime(data['event_date']) # set date column to datetime format to extract year\n",
    "    #    data['year'] = data['event_date'].dt.year\n",
    "    #    data['month']= data['event_date'].dt.month\n",
    "\n",
    "        #set date column to datetime format and extract year\n",
    "    #    data['event_date'] = pd.to_datetime(data['event_date'])\n",
    "    #    data['year'] = data['event_date'].dt.year\n",
    "    #    data['month']= data['event_date'].dt.month\n",
    "\n",
    "        #only include observations >1900\n",
    "    #    data3=data[data.year >= 1900]\n",
    "    #    print(\"length observationas >1900\", len(data3.index))\n",
    "        \n",
    "        \n",
    "        # check >10 observations\n",
    "        if len(data.index)>=10:\n",
    "            #save to csv\n",
    "            data.to_csv(file_dir+'/modified_data/gbif_filtered/%s_filtered_data.csv'%spec)\n",
    "            taxa_list.write(spec+\"\\n\")\n",
    "            \n",
    "#close text file\n",
    "taxa_list.close()\n",
    "# next species!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
